{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RMI Example - People Dataset\n",
      "============================================================\n",
      "Index created with 7 elements\n",
      "Max Error Bound: 1\n",
      "Min Error Bound: -2\n",
      "\n",
      "People who are 23.0 years old are located at [1, 6]\n",
      "\n",
      "\n",
      "For comprehensive benchmarks, load your dataset:\n",
      "  dataset = extract_column('path/to/titanic.csv', 'age')\n",
      "  run_tests(dataset)\n",
      "  run_benchmarks(dataset, n_queries=10000)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RMI (Recursive Model Index) Implementation in Python\n",
    "Translated from the Go implementation at github.com/BenJoyenConseil/rmi\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Part 1: Linear Regression Estimator\n",
    "# Translated from: estimate/linear/estimator.go\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Optional\n",
    "import time\n",
    "\n",
    "class RegressionModel:\n",
    "    \"\"\"Linear Regression model for CDF prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, intercept: float, slope: float):\n",
    "        self.intercept = intercept\n",
    "        self.slope = slope\n",
    "    \n",
    "    def predict(self, x: float) -> float:\n",
    "        \"\"\"Predict the CDF result of a given x\"\"\"\n",
    "        return self.intercept + self.slope * x\n",
    "\n",
    "\n",
    "def fit(x: np.ndarray, y: np.ndarray) -> RegressionModel:\n",
    "    \"\"\"\n",
    "    Fit a linear regression model on x and y\n",
    "    Returns a RegressionModel with alpha (intercept) and beta (slope)\n",
    "    \"\"\"\n",
    "    # Using scipy's linregress for linear regression\n",
    "    slope, intercept, _, _, _ = stats.linregress(x, y)\n",
    "    \n",
    "    # Handle NaN cases (when all x values are the same)\n",
    "    if np.isnan(intercept) or np.isnan(slope):\n",
    "        intercept = np.mean(y)\n",
    "        slope = 0.0\n",
    "    \n",
    "    return RegressionModel(intercept, slope)\n",
    "\n",
    "\n",
    "def cdf(x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return the x array and the y array containing empirical CDF value for each x's value\n",
    "    len(x) = len(y)\n",
    "    \"\"\"\n",
    "    y = np.zeros(len(x))\n",
    "    for i, val in enumerate(x):\n",
    "        # Calculate empirical CDF for each value\n",
    "        y[i] = np.sum(x <= val) / len(x)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 2: Sorted Table\n",
    "# Translated from: search/sort.go\n",
    "# ============================================================\n",
    "\n",
    "class SortedTable:\n",
    "    \"\"\"\n",
    "    A Sorted Table represents a collection of key:offset pairs \n",
    "    that is sorted by key, keeping offsets following their corresponding key\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: np.ndarray, offsets: np.ndarray):\n",
    "        self.keys = keys\n",
    "        self.offsets = offsets\n",
    "\n",
    "\n",
    "def new_sorted_table(x: np.ndarray) -> SortedTable:\n",
    "    \"\"\"\n",
    "    Return a Sorted Table structure sorted by key in ascending order\n",
    "    \"\"\"\n",
    "    keys = x.copy()\n",
    "    offsets = np.arange(len(x))\n",
    "    \n",
    "    # Sort both arrays by keys\n",
    "    sort_indices = np.argsort(keys)\n",
    "    keys = keys[sort_indices]\n",
    "    offsets = offsets[sort_indices]\n",
    "    \n",
    "    return SortedTable(keys, offsets)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 3: Index Utilities\n",
    "# Translated from: index/bound.go\n",
    "# ============================================================\n",
    "\n",
    "def residual(guess: int, y: int) -> int:\n",
    "    \"\"\"Calculate residual between guess and actual position\"\"\"\n",
    "    return y - guess\n",
    "\n",
    "\n",
    "def scale(cdf_val: float, dataset_len: int) -> int:\n",
    "    \"\"\"\n",
    "    Scale returns the CDF value * datasetLen - 1 to get back \n",
    "    the position in a sortedTable\n",
    "    \"\"\"\n",
    "    return int(np.round(cdf_val * dataset_len - 1))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 4: Learned Index\n",
    "# Translated from: index/learn.go\n",
    "# ============================================================\n",
    "\n",
    "class LearnedIndex:\n",
    "    \"\"\"\n",
    "    LearnedIndex is an index structure that uses inference to locate keys\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m: RegressionModel, st: SortedTable, length: int, \n",
    "                 min_err_bound: int, max_err_bound: int):\n",
    "        self.m = m\n",
    "        self.st = st\n",
    "        self.len = length\n",
    "        self.min_err_bound = min_err_bound\n",
    "        self.max_err_bound = max_err_bound\n",
    "    \n",
    "    def guess_index(self, key: float) -> Tuple[int, int, int]:\n",
    "        \"\"\"\n",
    "        Return the predicted position of the key in the index\n",
    "        and upper/lower positions' search interval. \n",
    "        Guess, lower and upper always have values between 0 and len(keys)-1\n",
    "        \"\"\"\n",
    "        guess = scale(self.m.predict(key), self.len)\n",
    "        lower = self.min_err_bound + guess\n",
    "        \n",
    "        if lower < 0:\n",
    "            lower = 0\n",
    "        elif lower > self.len - 1:\n",
    "            lower = self.len - 1\n",
    "        \n",
    "        upper = guess + self.max_err_bound\n",
    "        if upper > self.len - 1:\n",
    "            upper = self.len - 1\n",
    "        elif upper < 0:\n",
    "            upper = 0\n",
    "        \n",
    "        if guess < 0:\n",
    "            guess = 0\n",
    "        elif guess > self.len - 1:\n",
    "            guess = self.len - 1\n",
    "        \n",
    "        return guess, lower, upper\n",
    "    \n",
    "    def lookup(self, key: float) -> Tuple[Optional[List[int]], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Lookup returns the offsets of the key or error if the key is not found\n",
    "        \"\"\"\n",
    "        guess, lower, upper = self.guess_index(key)\n",
    "        offsets = []\n",
    "        \n",
    "        # Binary search in the bounded range\n",
    "        if key > self.st.keys[guess]:\n",
    "            sub_keys = self.st.keys[guess + 1:upper + 1]\n",
    "            i = np.searchsorted(sub_keys, key, side='left') + guess + 1\n",
    "        elif key <= self.st.keys[guess]:\n",
    "            sub_keys = self.st.keys[lower:guess + 1]\n",
    "            i = np.searchsorted(sub_keys, key, side='left') + lower\n",
    "        \n",
    "        # Iterate to get all equal keys\n",
    "        while i <= upper:\n",
    "            if i < len(self.st.keys) and self.st.keys[i] == key:\n",
    "                offsets.append(int(self.st.offsets[i]))\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if len(offsets) == 0:\n",
    "            return None, f\"The following key <{key}> is not found in the index\"\n",
    "        \n",
    "        return offsets, None\n",
    "\n",
    "\n",
    "def new_learned_index(dataset: np.ndarray) -> LearnedIndex:\n",
    "    \"\"\"\n",
    "    Create a new LearnedIndex fitted over the dataset with linear regression\n",
    "    \"\"\"\n",
    "    st = new_sorted_table(dataset)\n",
    "    \n",
    "    x, y = cdf(st.keys)\n",
    "    len_ = len(dataset)\n",
    "    m = fit(x, y)\n",
    "    \n",
    "    guesses = np.zeros(len_, dtype=int)\n",
    "    scaled_y = np.zeros(len_, dtype=int)\n",
    "    max_err, min_err = 0, 0\n",
    "    \n",
    "    for i, k in enumerate(x):\n",
    "        guesses[i] = scale(m.predict(k), len_)\n",
    "        scaled_y[i] = scale(y[i], len_)\n",
    "        res = residual(guesses[i], scaled_y[i])\n",
    "        \n",
    "        if res > max_err:\n",
    "            max_err = res\n",
    "        elif res < min_err:\n",
    "            min_err = res\n",
    "    \n",
    "    return LearnedIndex(m, st, len_, min_err, max_err)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 5: Search Algorithms for Comparison\n",
    "# Translated from: search/fullscan.go and search/binary.go\n",
    "# ============================================================\n",
    "\n",
    "def full_scan_lookup(key: float, st: SortedTable) -> Tuple[Optional[List[int]], Optional[str]]:\n",
    "    \"\"\"Full scan lookup for comparison\"\"\"\n",
    "    offsets = []\n",
    "    \n",
    "    for i in range(len(st.keys)):\n",
    "        if st.keys[i] == key:\n",
    "            offsets.append(int(st.offsets[i]))\n",
    "    \n",
    "    if len(offsets) > 0:\n",
    "        return offsets, None\n",
    "    return None, f\"The following key <{key}> is not found in the index\"\n",
    "\n",
    "\n",
    "def binary_search_lookup(key: float, st: SortedTable) -> Tuple[Optional[List[int]], Optional[str]]:\n",
    "    \"\"\"Binary search lookup for comparison\"\"\"\n",
    "    i = np.searchsorted(st.keys, key, side='left')\n",
    "    offsets = []\n",
    "    \n",
    "    while i < len(st.keys):\n",
    "        if st.keys[i] > key:\n",
    "            break\n",
    "        elif st.keys[i] == key:\n",
    "            offsets.append(int(st.offsets[i]))\n",
    "        i += 1\n",
    "    \n",
    "    if len(offsets) > 0:\n",
    "        return offsets, None\n",
    "    return None, f\"The following key <{key}> is not found in the index\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 6: Benchmarking Functions\n",
    "# Translated from: functional_test.go (benchmark sections)\n",
    "# ============================================================\n",
    "\n",
    "def benchmark_learned_index(dataset: np.ndarray, n_queries: int = 10000, \n",
    "                           key_range: Tuple[float, float] = (0., 100.)) -> dict:\n",
    "    \"\"\"Benchmark the learned index\"\"\"\n",
    "    idx = new_learned_index(dataset)\n",
    "    \n",
    "    min_val, max_val = key_range\n",
    "    keys_found = {}\n",
    "    keys_not_found = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(n_queries):\n",
    "        k = np.round(min_val + np.random.random() * (max_val - min_val))\n",
    "        offsets, err = idx.lookup(k)\n",
    "        \n",
    "        if err is not None:\n",
    "            if k not in keys_not_found:\n",
    "                keys_not_found[k] = []\n",
    "            keys_not_found[k].append(err)\n",
    "        else:\n",
    "            keys_found[k] = offsets\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'time': elapsed_time,\n",
    "        'queries': n_queries,\n",
    "        'keys_found': len(keys_found),\n",
    "        'keys_not_found': len(keys_not_found),\n",
    "        'time_per_query': elapsed_time / n_queries * 1e6  # microseconds\n",
    "    }\n",
    "\n",
    "\n",
    "def benchmark_binary_search(dataset: np.ndarray, n_queries: int = 10000,\n",
    "                           key_range: Tuple[float, float] = (0., 100.)) -> dict:\n",
    "    \"\"\"Benchmark binary search\"\"\"\n",
    "    st = new_sorted_table(dataset)\n",
    "    \n",
    "    min_val, max_val = key_range\n",
    "    keys_found = {}\n",
    "    keys_not_found = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(n_queries):\n",
    "        k = np.round(min_val + np.random.random() * (max_val - min_val))\n",
    "        offsets, err = binary_search_lookup(k, st)\n",
    "        \n",
    "        if err is not None:\n",
    "            if k not in keys_not_found:\n",
    "                keys_not_found[k] = []\n",
    "            keys_not_found[k].append(err)\n",
    "        else:\n",
    "            keys_found[k] = offsets\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'time': elapsed_time,\n",
    "        'queries': n_queries,\n",
    "        'keys_found': len(keys_found),\n",
    "        'keys_not_found': len(keys_not_found),\n",
    "        'time_per_query': elapsed_time / n_queries * 1e6  # microseconds\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 7: Example Usage and Testing\n",
    "# Translated from: main.go and functional_test.go\n",
    "# ============================================================\n",
    "\n",
    "def extract_column(file_path: str, col_name: str) -> np.ndarray:\n",
    "    \"\"\"Extract a column from CSV file\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df[col_name].values\n",
    "\n",
    "\n",
    "def run_example():\n",
    "    \"\"\"Run the example from main.go\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RMI Example - People Dataset\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create sample data (from data/people.csv)\n",
    "    people_data = {\n",
    "        'name': ['jeanne', 'jean', 'Carlos', 'Carlotta', 'Miguel', 'Martine', 'Georgette'],\n",
    "        'age': [90, 23, 3, 45, 1, 1.5, 23],\n",
    "        'sex': ['F', 'M', 'M', 'F', 'M', 'F', 'F']\n",
    "    }\n",
    "    \n",
    "    age_column = np.array(people_data['age'])\n",
    "    \n",
    "    # Create index\n",
    "    index = new_learned_index(age_column)\n",
    "    \n",
    "    print(f\"Index created with {index.len} elements\")\n",
    "    print(f\"Max Error Bound: {index.max_err_bound}\")\n",
    "    print(f\"Min Error Bound: {index.min_err_bound}\")\n",
    "    print()\n",
    "    \n",
    "    # Search for age 23\n",
    "    search_age = 23.0\n",
    "    lines, err = index.lookup(search_age)\n",
    "    \n",
    "    if err:\n",
    "        print(f\"Error: {err}\")\n",
    "    else:\n",
    "        print(f\"People who are {search_age} years old are located at {lines}\")\n",
    "    \n",
    "    return index, age_column\n",
    "\n",
    "\n",
    "def run_tests(dataset: np.ndarray):\n",
    "    \"\"\"Run functional tests to verify correctness\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Functional Tests - Verifying Correctness\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    li = new_learned_index(dataset)\n",
    "    st = new_sorted_table(dataset)\n",
    "    \n",
    "    # Test for all unique keys\n",
    "    unique_keys = np.unique(dataset)\n",
    "    all_passed = True\n",
    "    \n",
    "    for key in unique_keys:\n",
    "        result_fs, _ = full_scan_lookup(key, st)\n",
    "        result_li, _ = li.lookup(key)\n",
    "        \n",
    "        if set(result_fs) != set(result_li):\n",
    "            print(f\"FAIL: Key {key} - FS: {result_fs}, LI: {result_li}\")\n",
    "            all_passed = False\n",
    "    \n",
    "    if all_passed:\n",
    "        print(f\"✓ All {len(unique_keys)} keys passed functional tests\")\n",
    "    else:\n",
    "        print(\"✗ Some tests failed\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "\n",
    "def run_benchmarks(dataset: np.ndarray, n_queries: int = 10000):\n",
    "    \"\"\"Run benchmarks comparing learned index vs binary search\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Benchmarks - {n_queries} queries\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    key_min, key_max = dataset.min(), dataset.max()\n",
    "    \n",
    "    # Benchmark Learned Index\n",
    "    print(\"\\nRunning Learned Index benchmark...\")\n",
    "    li_results = benchmark_learned_index(dataset, n_queries, (key_min, key_max))\n",
    "    \n",
    "    # Benchmark Binary Search\n",
    "    print(\"Running Binary Search benchmark...\")\n",
    "    bs_results = benchmark_binary_search(dataset, n_queries, (key_min, key_max))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"LEARNED INDEX:\")\n",
    "    print(f\"  Total time: {li_results['time']:.4f} seconds\")\n",
    "    print(f\"  Time per query: {li_results['time_per_query']:.2f} μs\")\n",
    "    print(f\"  Keys found: {li_results['keys_found']}\")\n",
    "    print(f\"  Keys not found: {li_results['keys_not_found']}\")\n",
    "    \n",
    "    print(\"\\nBINARY SEARCH:\")\n",
    "    print(f\"  Total time: {bs_results['time']:.4f} seconds\")\n",
    "    print(f\"  Time per query: {bs_results['time_per_query']:.2f} μs\")\n",
    "    print(f\"  Keys found: {bs_results['keys_found']}\")\n",
    "    print(f\"  Keys not found: {bs_results['keys_not_found']}\")\n",
    "    \n",
    "    speedup = bs_results['time'] / li_results['time']\n",
    "    print(f\"\\nSpeedup: {speedup:.2f}x\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    return li_results, bs_results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main execution\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the basic example\n",
    "    run_example()\n",
    "    \n",
    "    # For more comprehensive testing, load a larger dataset\n",
    "    print(\"\\n\\nFor comprehensive benchmarks, load your dataset:\")\n",
    "    print(\"  dataset = extract_column('path/to/titanic.csv', 'age')\")\n",
    "    print(\"  run_tests(dataset)\")\n",
    "    print(\"  run_benchmarks(dataset, n_queries=10000)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
