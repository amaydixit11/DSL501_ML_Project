{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e0132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RMI (Recursive Model Index) Implementation\n",
      "Translated from Rust implementation\n",
      "======================================================================\n",
      "\n",
      "1. Creating synthetic dataset...\n",
      "Created 1000000 data points\n",
      "\n",
      "======================================================================\n",
      "2. Training RMI: linear,linear with branching factor 1000\n",
      "======================================================================\n",
      "Training top-level linear model...\n",
      "Training second-level linear models (num models = 1000)...\n",
      "Computing lower bound corrections...\n",
      "Fixing empty models...\n",
      "Computing last level errors...\n",
      "Evaluating RMI...\n",
      "\n",
      "RMI Training Complete!\n",
      "Build time: 5.543s\n",
      "Average error: 1841.99\n",
      "Max error: 38888\n",
      "Average log2 error: 6.352\n",
      "Max log2 error: 15.247\n",
      "\n",
      "3. Testing predictions...\n",
      "\n",
      "=== Benchmarking RMI ===\n",
      "Tested 1000 keys\n",
      "Average search range: 3015.47\n",
      "Prediction accuracy: 99.80%\n",
      "Avg log2(search range): 11.559\n",
      "\n",
      "======================================================================\n",
      "2. Training RMI: robust_linear,linear with branching factor 1000\n",
      "======================================================================\n",
      "Training top-level robust_linear model...\n",
      "Training second-level linear models (num models = 1000)...\n",
      "Computing lower bound corrections...\n",
      "Fixing empty models...\n",
      "Computing last level errors...\n",
      "Evaluating RMI...\n",
      "\n",
      "RMI Training Complete!\n",
      "Build time: 13.794s\n",
      "Average error: 1864.63\n",
      "Max error: 39129\n",
      "Average log2 error: 6.357\n",
      "Max log2 error: 15.256\n",
      "\n",
      "3. Testing predictions...\n",
      "\n",
      "=== Benchmarking RMI ===\n",
      "Tested 1000 keys\n",
      "Average search range: 3099.41\n",
      "Prediction accuracy: 100.00%\n",
      "Avg log2(search range): 11.598\n",
      "\n",
      "======================================================================\n",
      "2. Training RMI: cubic,linear with branching factor 1000\n",
      "======================================================================\n",
      "Training top-level cubic model...\n",
      "Training second-level linear models (num models = 1000)...\n",
      "Computing lower bound corrections...\n",
      "Fixing empty models...\n",
      "Computing last level errors...\n",
      "Evaluating RMI...\n",
      "\n",
      "RMI Training Complete!\n",
      "Build time: 10.371s\n",
      "Average error: 31.37\n",
      "Max error: 67\n",
      "Average log2 error: 5.934\n",
      "Max log2 error: 6.066\n",
      "\n",
      "3. Testing predictions...\n",
      "\n",
      "=== Benchmarking RMI ===\n",
      "Tested 1000 keys\n",
      "Average search range: 63.80\n",
      "Prediction accuracy: 100.00%\n",
      "Avg log2(search range): 6.018\n",
      "\n",
      "======================================================================\n",
      "2. Training RMI: linear_spline,linear with branching factor 1000\n",
      "======================================================================\n",
      "Training top-level linear_spline model...\n",
      "Training second-level linear models (num models = 1000)...\n",
      "Computing lower bound corrections...\n",
      "Fixing empty models...\n",
      "Computing last level errors...\n",
      "Evaluating RMI...\n",
      "\n",
      "RMI Training Complete!\n",
      "Build time: 6.189s\n",
      "Average error: 37.02\n",
      "Max error: 1000000\n",
      "Average log2 error: 6.159\n",
      "Max log2 error: 19.932\n",
      "\n",
      "3. Testing predictions...\n",
      "\n",
      "=== Benchmarking RMI ===\n",
      "Tested 1000 keys\n",
      "Average search range: 74.63\n",
      "Prediction accuracy: 100.00%\n",
      "Avg log2(search range): 6.241\n",
      "\n",
      "======================================================================\n",
      "RESULTS COMPARISON\n",
      "======================================================================\n",
      "Config                    Branch   Build(s)   AvgErr     MaxErr     AvgLog2    SearchRng \n",
      "----------------------------------------------------------------------\n",
      "linear,linear             1000     5.543      1841.99    38888      6.352      3015.47   \n",
      "robust_linear,linear      1000     13.794     1864.63    39129      6.357      3099.41   \n",
      "cubic,linear              1000     10.371     31.37      67         5.934      63.80     \n",
      "linear_spline,linear      1000     6.189      37.02      1000000    6.159      74.63     \n",
      "\n",
      "======================================================================\n",
      "Done!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RMI (Recursive Model Index) Implementation in Python\n",
    "Rust implementation at: https://github.com/learnedsystems/RMI\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional, Union, Iterator, Callable\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import math\n",
    "from enum import Enum\n",
    "import bisect\n",
    "\n",
    "# =============================================================================\n",
    "# From: rmi_lib/src/models/mod.rs\n",
    "# Model base classes and type definitions\n",
    "# =============================================================================\n",
    "\n",
    "class ModelDataType(Enum):\n",
    "    INT = \"uint64_t\"\n",
    "    INT128 = \"uint128_t\"\n",
    "    FLOAT = \"double\"\n",
    "\n",
    "class ModelInput:\n",
    "    \"\"\"From: rmi_lib/src/models/mod.rs\"\"\"\n",
    "    def __init__(self, value: Union[int, float]):\n",
    "        if isinstance(value, (int, np.integer)):\n",
    "            self.value = int(value)\n",
    "            self.is_float = False\n",
    "        else:\n",
    "            self.value = float(value)\n",
    "            self.is_float = True\n",
    "    \n",
    "    def as_float(self) -> float:\n",
    "        return float(self.value)\n",
    "    \n",
    "    def as_int(self) -> int:\n",
    "        return int(self.value)\n",
    "\n",
    "class RMITrainingData:\n",
    "    \"\"\"\n",
    "    From: rmi_lib/src/models/mod.rs\n",
    "    Training data container for RMI\n",
    "    \"\"\"\n",
    "    def __init__(self, data: List[Tuple[Union[int, float], int]]):\n",
    "        self.data = sorted(data, key=lambda x: x[0])\n",
    "        self.scale = 1.0\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def set_scale(self, scale: float):\n",
    "        self.scale = scale\n",
    "    \n",
    "    def get(self, idx: int) -> Tuple[Union[int, float], int]:\n",
    "        key, offset = self.data[idx]\n",
    "        if abs(self.scale - 1.0) > 1e-10:\n",
    "            return (key, int(offset * self.scale))\n",
    "        return (key, offset)\n",
    "    \n",
    "    def get_key(self, idx: int) -> Union[int, float]:\n",
    "        return self.data[idx][0]\n",
    "    \n",
    "    def iter(self) -> Iterator[Tuple[Union[int, float], int]]:\n",
    "        \"\"\"Iterator with scale applied\"\"\"\n",
    "        for key, offset in self.data:\n",
    "            if abs(self.scale - 1.0) > 1e-10:\n",
    "                yield (key, int(offset * self.scale))\n",
    "            else:\n",
    "                yield (key, offset)\n",
    "    \n",
    "    def iter_model_input(self) -> Iterator[Tuple[ModelInput, int]]:\n",
    "        \"\"\"Iterator yielding ModelInput objects\"\"\"\n",
    "        for key, offset in self.iter():\n",
    "            yield (ModelInput(key), offset)\n",
    "    \n",
    "    def iter_unique(self) -> Iterator[Tuple[Union[int, float], int]]:\n",
    "        \"\"\"Iterator that removes duplicate keys\"\"\"\n",
    "        if len(self.data) == 0:\n",
    "            return\n",
    "        last_key = None\n",
    "        for key, offset in self.iter():\n",
    "            if last_key is None or key != last_key:\n",
    "                yield (key, offset)\n",
    "                last_key = key\n",
    "    \n",
    "    def lower_bound_by(self, cmp_func: Callable) -> int:\n",
    "        \"\"\"From: rmi_lib/src/models/mod.rs - binary search for lower bound\"\"\"\n",
    "        size = len(self)\n",
    "        if size == 0:\n",
    "            return 0\n",
    "        \n",
    "        base = 0\n",
    "        while size > 1:\n",
    "            half = size // 2\n",
    "            mid = base + half\n",
    "            cmp_result = cmp_func(self.get(mid))\n",
    "            if cmp_result < 0:  # Less\n",
    "                base = mid\n",
    "            size -= half\n",
    "        \n",
    "        cmp_result = cmp_func(self.get(base))\n",
    "        base += (1 if cmp_result < 0 else 0)\n",
    "        return base\n",
    "    \n",
    "    def soft_copy(self):\n",
    "        \"\"\"Create a shallow copy with same data\"\"\"\n",
    "        new_data = RMITrainingData(self.data[:])\n",
    "        new_data.scale = self.scale\n",
    "        return new_data\n",
    "\n",
    "class Model(ABC):\n",
    "    \"\"\"\n",
    "    From: rmi_lib/src/models/mod.rs\n",
    "    Base class for all RMI models\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def predict_to_float(self, inp: ModelInput) -> float:\n",
    "        pass\n",
    "    \n",
    "    def predict_to_int(self, inp: ModelInput) -> int:\n",
    "        return max(0, int(math.floor(self.predict_to_float(inp))))\n",
    "    \n",
    "    @abstractmethod\n",
    "    def input_type(self) -> ModelDataType:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def output_type(self) -> ModelDataType:\n",
    "        pass\n",
    "    \n",
    "    def needs_bounds_check(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def set_to_constant_model(self, constant: int) -> bool:\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# From: rmi_lib/src/models/linear.rs\n",
    "# Linear regression models\n",
    "# =============================================================================\n",
    "\n",
    "class LinearModel(Model):\n",
    "    \"\"\"From: rmi_lib/src/models/linear.rs\"\"\"\n",
    "    def __init__(self, data: RMITrainingData):\n",
    "        self.intercept, self.slope = self._slr(data)\n",
    "    \n",
    "    def _slr(self, data: RMITrainingData) -> Tuple[float, float]:\n",
    "        \"\"\"Simple linear regression using online algorithm\"\"\"\n",
    "        mean_x = 0.0\n",
    "        mean_y = 0.0\n",
    "        c = 0.0\n",
    "        n = 0\n",
    "        m2 = 0.0\n",
    "        data_size = 0\n",
    "        \n",
    "        for x, y in data.iter():\n",
    "            n += 1\n",
    "            dx = float(x) - mean_x\n",
    "            mean_x += dx / n\n",
    "            mean_y += (float(y) - mean_y) / n\n",
    "            c += dx * (float(y) - mean_y)\n",
    "            \n",
    "            dx2 = float(x) - mean_x\n",
    "            m2 += dx * dx2\n",
    "            data_size += 1\n",
    "        \n",
    "        if data_size == 0:\n",
    "            return (0.0, 0.0)\n",
    "        if data_size == 1:\n",
    "            return (mean_y, 0.0)\n",
    "        \n",
    "        cov = c / (n - 1)\n",
    "        var = m2 / (n - 1)\n",
    "        \n",
    "        if var == 0.0:\n",
    "            return (mean_y, 0.0)\n",
    "        \n",
    "        beta = cov / var\n",
    "        alpha = mean_y - beta * mean_x\n",
    "        \n",
    "        return (alpha, beta)\n",
    "    \n",
    "    def predict_to_float(self, inp: ModelInput) -> float:\n",
    "        return self.slope * inp.as_float() + self.intercept\n",
    "    \n",
    "    def input_type(self) -> ModelDataType:\n",
    "        return ModelDataType.FLOAT\n",
    "    \n",
    "    def output_type(self) -> ModelDataType:\n",
    "        return ModelDataType.FLOAT\n",
    "    \n",
    "    def set_to_constant_model(self, constant: int) -> bool:\n",
    "        self.intercept = float(constant)\n",
    "        self.slope = 0.0\n",
    "        return True\n",
    "\n",
    "class RobustLinearModel(Model):\n",
    "    \"\"\"From: rmi_lib/src/models/linear.rs\"\"\"\n",
    "    def __init__(self, data: RMITrainingData):\n",
    "        total_items = len(data)\n",
    "        if total_items == 0:\n",
    "            self.intercept, self.slope = (0.0, 0.0)\n",
    "            return\n",
    "        \n",
    "        # Skip 0.01% of data from each end for robustness\n",
    "        bnd = max(1, int(total_items * 0.0001))\n",
    "        \n",
    "        # Need at least bnd*2+1 items\n",
    "        if bnd * 2 + 1 >= total_items:\n",
    "            # Not enough data, use regular linear\n",
    "            self.intercept, self.slope = LinearModel(data)._slr(data)\n",
    "            return\n",
    "        \n",
    "        # Create iterator skipping first and last bnd items\n",
    "        subset = []\n",
    "        for i, item in enumerate(data.iter()):\n",
    "            if i < bnd:\n",
    "                continue\n",
    "            if i >= total_items - bnd:\n",
    "                break\n",
    "            subset.append(item)\n",
    "        \n",
    "        subset_data = RMITrainingData(subset)\n",
    "        self.intercept, self.slope = LinearModel(subset_data)._slr(subset_data)\n",
    "    \n",
    "    def predict_to_float(self, inp: ModelInput) -> float:\n",
    "        return self.slope * inp.as_float() + self.intercept\n",
    "    \n",
    "    def input_type(self) -> ModelDataType:\n",
    "        return ModelDataType.FLOAT\n",
    "    \n",
    "    def output_type(self) -> ModelDataType:\n",
    "        return ModelDataType.FLOAT\n",
    "    \n",
    "    def set_to_constant_model(self, constant: int) -> bool:\n",
    "        self.intercept = float(constant)\n",
    "        self.slope = 0.0\n",
    "        return True\n",
    "\n",
    "# =============================================================================\n",
    "# From: rmi_lib/src/models/linear_spline.rs\n",
    "# Linear spline model\n",
    "# =============================================================================\n",
    "\n",
    "class LinearSplineModel(Model):\n",
    "    \"\"\"From: rmi_lib/src/models/linear_spline.rs\"\"\"\n",
    "    def __init__(self, data: RMITrainingData):\n",
    "        self.intercept, self.slope = self._linear_splines(data)\n",
    "    \n",
    "    def _linear_splines(self, data: RMITrainingData) -> Tuple[float, float]:\n",
    "        if len(data) == 0:\n",
    "            return (0.0, 0.0)\n",
    "        if len(data) == 1:\n",
    "            return (float(data.get(0)[1]), 0.0)\n",
    "        \n",
    "        first_pt = data.get(0)\n",
    "        last_pt = data.get(len(data) - 1)\n",
    "        \n",
    "        if first_pt[0] == last_pt[0]:\n",
    "            return (float(data.get(0)[1]), 0.0)\n",
    "        \n",
    "        slope = (float(first_pt[1]) - float(last_pt[1])) / (float(first_pt[0]) - float(last_pt[0]))\n",
    "        intercept = float(first_pt[1]) - slope * float(first_pt[0])\n",
    "        \n",
    "        return (intercept, slope)\n",
    "    \n",
    "    def predict_to_float(self, inp: ModelInput) -> float:\n",
    "        return self.slope * inp.as_float() + self.intercept\n",
    "    \n",
    "    def input_type(self) -> ModelDataType:\n",
    "        return ModelDataType.FLOAT\n",
    "    \n",
    "    def output_type(self) -> ModelDataType:\n",
    "        return ModelDataType.FLOAT\n",
    "    \n",
    "    def set_to_constant_model(self, constant: int) -> bool:\n",
    "        self.intercept = float(constant)\n",
    "        self.slope = 0.0\n",
    "        return True\n",
    "\n",
    "# =============================================================================\n",
    "# From: rmi_lib/src/models/cubic_spline.rs\n",
    "# Cubic spline model\n",
    "# =============================================================================\n",
    "\n",
    "class CubicSplineModel(Model):\n",
    "    \"\"\"From: rmi_lib/src/models/cubic_spline.rs\"\"\"\n",
    "    def __init__(self, data: RMITrainingData):\n",
    "        self.a, self.b, self.c, self.d = self._cubic(data)\n",
    "        \n",
    "        # Check against linear model - sometimes cubic doesn't work well\n",
    "        linear = LinearSplineModel(data)\n",
    "        \n",
    "        our_error = 0.0\n",
    "        lin_error = 0.0\n",
    "        \n",
    "        for x, y in data.iter_model_input():\n",
    "            c_pred = self.predict_to_float(x)\n",
    "            l_pred = linear.predict_to_float(x)\n",
    "            \n",
    "            our_error += abs(c_pred - float(y))\n",
    "            lin_error += abs(l_pred - float(y))\n",
    "        \n",
    "        if lin_error < our_error:\n",
    "            # Use linear instead\n",
    "            self.a = 0.0\n",
    "            self.b = 0.0\n",
    "            self.c = linear.slope\n",
    "            self.d = linear.intercept\n",
    "    \n",
    "    def _cubic(self, data: RMITrainingData) -> Tuple[float, float, float, float]:\n",
    "        if len(data) == 0:\n",
    "            return (0.0, 0.0, 1.0, 0.0)\n",
    "        if len(data) == 1:\n",
    "            return (0.0, 0.0, 0.0, float(data.get(0)[1]))\n",
    "        \n",
    "        # Check for unique values\n",
    "        candidate = data.get(0)[0]\n",
    "        uniq = any(x != candidate for x, _ in data.iter())\n",
    "        if not uniq:\n",
    "            return (0.0, 0.0, 0.0, float(data.get(0)[1]))\n",
    "        \n",
    "        first_pt = data.get(0)\n",
    "        last_pt = data.get(len(data) - 1)\n",
    "        xmin, ymin = float(first_pt[0]), float(first_pt[1])\n",
    "        xmax, ymax = float(last_pt[0]), float(last_pt[1])\n",
    "        \n",
    "        x1, y1 = 0.0, 0.0\n",
    "        x2, y2 = 1.0, 1.0\n",
    "        \n",
    "        # Find first point with scaled x > 0\n",
    "        m1 = None\n",
    "        for xn, yn in data.iter():\n",
    "            sxn = (float(xn) - xmin) / (xmax - xmin)\n",
    "            if sxn > 0.0:\n",
    "                syn = (float(yn) - ymin) / (ymax - ymin)\n",
    "                m1 = (syn - y1) / (sxn - x1)\n",
    "                break\n",
    "        \n",
    "        if m1 is None:\n",
    "            m1 = 0.0\n",
    "        \n",
    "        # Find last point with scaled x < 1\n",
    "        m2 = None\n",
    "        for i in range(len(data) - 1, -1, -1):\n",
    "            xp, yp = data.get(i)\n",
    "            sxp = (float(xp) - xmin) / (xmax - xmin)\n",
    "            if sxp < 1.0:\n",
    "                syp = (float(yp) - ymin) / (ymax - ymin)\n",
    "                m2 = (y2 - syp) / (x2 - sxp)\n",
    "                break\n",
    "        \n",
    "        if m2 is None:\n",
    "            m2 = 0.0\n",
    "        \n",
    "        # Keep monotonic\n",
    "        if m1**2 + m2**2 > 9.0:\n",
    "            tau = 3.0 / math.sqrt(m1**2 + m2**2)\n",
    "            m1 *= tau\n",
    "            m2 *= tau\n",
    "        \n",
    "        # Compute coefficients\n",
    "        a = (m1 + m2 - 2.0) / (xmax - xmin)**3\n",
    "        b = -(xmax * (2.0*m1 + m2 - 3.0) + xmin * (m1 + 2.0*m2 - 3.0)) / (xmax - xmin)**3\n",
    "        c = (m1*xmax**2 + m2*xmin**2 + xmax*xmin*(2.0*m1 + 2.0*m2 - 6.0)) / (xmax - xmin)**3\n",
    "        d = -xmin * (m1*xmax**2 + xmax*xmin*(m2 - 3.0) + xmin**2) / (xmax - xmin)**3\n",
    "        \n",
    "        a *= ymax - ymin\n",
    "        b *= ymax - ymin\n",
    "        c *= ymax - ymin\n",
    "        d *= ymax - ymin\n",
    "        d += ymin\n",
    "        \n",
    "        return (a, b, c, d)\n",
    "    \n",
    "    def predict_to_float(self, inp: ModelInput) -> float:\n",
    "        val = inp.as_float()\n",
    "        # Use FMA-like computation\n",
    "        v1 = self.a * val + self.b\n",
    "        v2 = v1 * val + self.c\n",
    "        v3 = v2 * val + self.d\n",
    "        return v3\n",
    "    \n",
    "    def input_type(self) -> ModelDataType:\n",
    "        return ModelDataType.FLOAT\n",
    "    \n",
    "    def output_type(self) -> ModelDataType:\n",
    "        return ModelDataType.FLOAT\n",
    "    \n",
    "    def set_to_constant_model(self, constant: int) -> bool:\n",
    "        self.a = 0.0\n",
    "        self.b = 0.0\n",
    "        self.c = 0.0\n",
    "        self.d = float(constant)\n",
    "        return True\n",
    "\n",
    "# =============================================================================\n",
    "# From: rmi_lib/src/models/radix.rs\n",
    "# Radix models\n",
    "# =============================================================================\n",
    "\n",
    "def num_bits(largest_target: int) -> int:\n",
    "    \"\"\"From: rmi_lib/src/models/utils.rs\"\"\"\n",
    "    nbits = 0\n",
    "    while (1 << (nbits + 1)) - 1 <= largest_target:\n",
    "        nbits += 1\n",
    "    assert nbits >= 1\n",
    "    return nbits\n",
    "\n",
    "def common_prefix_size(data: RMITrainingData) -> int:\n",
    "    \"\"\"From: rmi_lib/src/models/utils.rs\"\"\"\n",
    "    any_ones = 0\n",
    "    no_ones = (1 << 64) - 1\n",
    "    \n",
    "    for x, _ in data.iter_model_input():\n",
    "        val = x.as_int()\n",
    "        any_ones |= val\n",
    "        no_ones &= val\n",
    "    \n",
    "    any_zeros = ~no_ones & ((1 << 64) - 1)\n",
    "    prefix_bits = any_zeros ^ any_ones\n",
    "    \n",
    "    # Count leading zeros\n",
    "    prefix_bits &= ((1 << 64) - 1)\n",
    "    if prefix_bits == 0:\n",
    "        return 64\n",
    "    \n",
    "    leading_zeros = 0\n",
    "    test_bit = 1 << 63\n",
    "    while test_bit > 0 and (~prefix_bits & test_bit):\n",
    "        leading_zeros += 1\n",
    "        test_bit >>= 1\n",
    "    \n",
    "    return leading_zeros\n",
    "\n",
    "class RadixModel(Model):\n",
    "    \"\"\"From: rmi_lib/src/models/radix.rs\"\"\"\n",
    "    def __init__(self, data: RMITrainingData):\n",
    "        if len(data) == 0:\n",
    "            self.left_shift = 0\n",
    "            self.num_bits = 0\n",
    "            return\n",
    "        \n",
    "        largest_value = max(y for _, y in data.iter())\n",
    "        bits = num_bits(largest_value)\n",
    "        common_prefix = common_prefix_size(data)\n",
    "        \n",
    "        self.left_shift = common_prefix\n",
    "        self.num_bits = bits\n",
    "    \n",
    "    def predict_to_int(self, inp: ModelInput) -> int:\n",
    "        as_int = inp.as_int()\n",
    "        res = (as_int << self.left_shift) >> (64 - self.num_bits)\n",
    "        return res & ((1 << 64) - 1)\n",
    "    \n",
    "    def predict_to_float(self, inp: ModelInput) -> float:\n",
    "        return float(self.predict_to_int(inp))\n",
    "    \n",
    "    def input_type(self) -> ModelDataType:\n",
    "        return ModelDataType.INT\n",
    "    \n",
    "    def output_type(self) -> ModelDataType:\n",
    "        return ModelDataType.INT\n",
    "    \n",
    "    def needs_bounds_check(self) -> bool:\n",
    "        return False\n",
    "\n",
    "class RadixTable(Model):\n",
    "    \"\"\"From: rmi_lib/src/models/radix.rs\"\"\"\n",
    "    def __init__(self, data: RMITrainingData, bits: int):\n",
    "        self.prefix_bits = common_prefix_size(data)\n",
    "        self.table_bits = bits\n",
    "        self.hint_table = [0] * (1 << bits)\n",
    "        \n",
    "        last_radix = 0\n",
    "        for inp, y in data.iter_model_input():\n",
    "            x = inp.as_int()\n",
    "            num_bits = 0 if self.prefix_bits + bits > 64 else 64 - (self.prefix_bits + bits)\n",
    "            current_radix = ((x << self.prefix_bits) >> self.prefix_bits) >> num_bits\n",
    "            \n",
    "            if current_radix == last_radix:\n",
    "                continue\n",
    "            \n",
    "            self.hint_table[int(current_radix)] = y\n",
    "            \n",
    "            for i in range(int(last_radix) + 1, int(current_radix)):\n",
    "                self.hint_table[i] = y\n",
    "            \n",
    "            last_radix = current_radix\n",
    "        \n",
    "        for i in range(int(last_radix) + 1, len(self.hint_table)):\n",
    "            self.hint_table[i] = len(self.hint_table)\n",
    "    \n",
    "    def predict_to_int(self, inp: ModelInput) -> int:\n",
    "        as_int = inp.as_int()\n",
    "        num_bits = 0 if self.prefix_bits + self.table_bits > 64 else 64 - (self.prefix_bits + self.table_bits)\n",
    "        res = ((as_int << self.prefix_bits) >> self.prefix_bits) >> num_bits\n",
    "        idx = self.hint_table[int(res)]\n",
    "        return idx\n",
    "    \n",
    "    def predict_to_float(self, inp: ModelInput) -> float:\n",
    "        return float(self.predict_to_int(inp))\n",
    "    \n",
    "    def input_type(self) -> ModelDataType:\n",
    "        return ModelDataType.INT\n",
    "    \n",
    "    def output_type(self) -> ModelDataType:\n",
    "        return ModelDataType.INT\n",
    "    \n",
    "    def needs_bounds_check(self) -> bool:\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# From: rmi_lib/src/train/lower_bound_correction.rs\n",
    "# Lower bound correction for empty models\n",
    "# =============================================================================\n",
    "\n",
    "class LowerBoundCorrection:\n",
    "    \"\"\"From: rmi_lib/src/train/lower_bound_correction.rs\"\"\"\n",
    "    def __init__(self, pred_func: Callable, num_leaf_models: int, data: RMITrainingData):\n",
    "        self.first = [None] * num_leaf_models\n",
    "        self.last = [None] * num_leaf_models\n",
    "        self.next = [(0, 0)] * num_leaf_models\n",
    "        self.prev = [(0, 0)] * num_leaf_models\n",
    "        self.run_lengths = [0] * num_leaf_models\n",
    "        \n",
    "        last_target = 0\n",
    "        current_run_length = 0\n",
    "        current_run_key = data.get_key(0)\n",
    "        \n",
    "        for x, y in data.iter():\n",
    "            leaf_idx = pred_func(x)\n",
    "            target = min(num_leaf_models - 1, leaf_idx)\n",
    "            \n",
    "            if target == last_target and x == current_run_key:\n",
    "                current_run_length += 1\n",
    "            elif target != last_target or x != current_run_key:\n",
    "                self.run_lengths[last_target] = max(\n",
    "                    self.run_lengths[last_target], current_run_length\n",
    "                )\n",
    "                current_run_length = 1\n",
    "                current_run_key = x\n",
    "                last_target = target\n",
    "            \n",
    "            if self.first[target] is None:\n",
    "                self.first[target] = (y, x)\n",
    "            self.last[target] = (y, x)\n",
    "        \n",
    "        # Compute next_for_leaf\n",
    "        idx = 0\n",
    "        while idx < num_leaf_models:\n",
    "            next_found = None\n",
    "            for i in range(idx + 1, num_leaf_models):\n",
    "                if self.first[i] is not None:\n",
    "                    next_found = (i, self.first[i])\n",
    "                    break\n",
    "            \n",
    "            if next_found:\n",
    "                next_leaf_idx, val = next_found\n",
    "                for i in range(idx, next_leaf_idx):\n",
    "                    self.next[i] = val\n",
    "                idx = next_leaf_idx\n",
    "            else:\n",
    "                for i in range(idx, num_leaf_models):\n",
    "                    self.next[i] = (len(data), data.get_key(len(data) - 1) if len(data) > 0 else 0)\n",
    "                break\n",
    "        \n",
    "        # Compute prev_for_leaf\n",
    "        idx = num_leaf_models - 1\n",
    "        while idx > 0:\n",
    "            prev_found = None\n",
    "            for i in range(idx - 1, -1, -1):\n",
    "                if self.last[i] is not None:\n",
    "                    prev_found = (i, self.last[i])\n",
    "                    break\n",
    "            \n",
    "            if prev_found:\n",
    "                prev_leaf_idx, val = prev_found\n",
    "                for i in range(prev_leaf_idx + 1, idx + 1):\n",
    "                    self.prev[i] = val\n",
    "                idx = prev_leaf_idx\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    def first_key(self, leaf_idx: int) -> Optional[Union[int, float]]:\n",
    "        return self.first[leaf_idx][1] if self.first[leaf_idx] else None\n",
    "    \n",
    "    def last_key(self, leaf_idx: int) -> Optional[Union[int, float]]:\n",
    "        return self.last[leaf_idx][1] if self.last[leaf_idx] else None\n",
    "    \n",
    "    def next_index(self, leaf_idx: int) -> int:\n",
    "        return self.next[leaf_idx][0]\n",
    "    \n",
    "    def prev_key(self, leaf_idx: int) -> Union[int, float]:\n",
    "        return self.prev[leaf_idx][1]\n",
    "    \n",
    "    def longest_run(self, leaf_idx: int) -> int:\n",
    "        return self.run_lengths[leaf_idx]\n",
    "\n",
    "# =============================================================================\n",
    "# From: rmi_lib/src/train/two_layer.rs\n",
    "# Two-layer RMI training\n",
    "# =============================================================================\n",
    "\n",
    "def error_between(v1: int, v2: int, max_pred: int) -> int:\n",
    "    \"\"\"From: rmi_lib/src/train/two_layer.rs\"\"\"\n",
    "    pred1 = min(v1, max_pred)\n",
    "    pred2 = min(v2, max_pred)\n",
    "    return max(pred1, pred2) - min(pred1, pred2)\n",
    "\n",
    "def train_model(model_type: str, data: RMITrainingData) -> Model:\n",
    "    \"\"\"From: rmi_lib/src/train/mod.rs\"\"\"\n",
    "    model_map = {\n",
    "        'linear': LinearModel,\n",
    "        'robust_linear': RobustLinearModel,\n",
    "        'linear_spline': LinearSplineModel,\n",
    "        'cubic': CubicSplineModel,\n",
    "        'radix': RadixModel,\n",
    "    }\n",
    "    \n",
    "    if model_type.startswith('radix') and model_type != 'radix':\n",
    "        bits = int(model_type[5:])\n",
    "        return RadixTable(data, bits)\n",
    "    \n",
    "    if model_type not in model_map:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    return model_map[model_type](data)\n",
    "\n",
    "@dataclass\n",
    "class TrainedRMI:\n",
    "    \"\"\"From: rmi_lib/src/train/mod.rs\"\"\"\n",
    "    num_rmi_rows: int\n",
    "    num_data_rows: int\n",
    "    model_avg_error: float\n",
    "    model_avg_l2_error: float\n",
    "    model_avg_log2_error: float\n",
    "    model_max_error: int\n",
    "    model_max_error_idx: int\n",
    "    model_max_log2_error: float\n",
    "    last_layer_max_l1s: List[int]\n",
    "    rmi: List[List[Model]]\n",
    "    models: str\n",
    "    branching_factor: int\n",
    "    build_time: float\n",
    "    \n",
    "    def predict(self, key: Union[int, float]) -> Tuple[int, int]:\n",
    "        \"\"\"Predict position and error for a key\"\"\"\n",
    "        inp = ModelInput(key)\n",
    "        \n",
    "        # First layer\n",
    "        model_idx = self.rmi[0][0].predict_to_int(inp)\n",
    "        \n",
    "        # Second layer\n",
    "        model_idx = min(len(self.rmi[1]) - 1, model_idx)\n",
    "        prediction = self.rmi[1][model_idx].predict_to_int(inp)\n",
    "        error = self.last_layer_max_l1s[model_idx]\n",
    "        \n",
    "        return (prediction, error)\n",
    "\n",
    "def build_models_from(\n",
    "    data: RMITrainingData,\n",
    "    top_model: Model,\n",
    "    model_type: str,\n",
    "    start_idx: int,\n",
    "    end_idx: int,\n",
    "    first_model_idx: int,\n",
    "    num_models: int\n",
    ") -> List[Model]:\n",
    "    \"\"\"From: rmi_lib/src/train/two_layer.rs\"\"\"\n",
    "    \n",
    "    assert end_idx > start_idx\n",
    "    assert end_idx <= len(data)\n",
    "    assert start_idx <= len(data)\n",
    "    \n",
    "    leaf_models = []\n",
    "    second_layer_data = []\n",
    "    last_target = first_model_idx\n",
    "    \n",
    "    # Get bounded iterator\n",
    "    for i, (x, y) in enumerate(data.iter()):\n",
    "        if i < start_idx:\n",
    "            continue\n",
    "        if i >= end_idx:\n",
    "            break\n",
    "        \n",
    "        model_pred = top_model.predict_to_int(ModelInput(x))\n",
    "        target = min(first_model_idx + num_models - 1, model_pred)\n",
    "        assert target >= last_target\n",
    "        \n",
    "        if target > last_target:\n",
    "            # Train previous model\n",
    "            last_item = second_layer_data[-1] if second_layer_data else None\n",
    "            second_layer_data.append((x, y))\n",
    "            \n",
    "            container = RMITrainingData(second_layer_data)\n",
    "            leaf_model = train_model(model_type, container)\n",
    "            leaf_models.append(leaf_model)\n",
    "            \n",
    "            # Add empty models for skipped indices\n",
    "            for _ in range(last_target + 1, target):\n",
    "                empty_data = RMITrainingData([])\n",
    "                leaf_models.append(train_model(model_type, empty_data))\n",
    "            \n",
    "            assert len(leaf_models) + first_model_idx == target\n",
    "            \n",
    "            second_layer_data = []\n",
    "            if last_item:\n",
    "                second_layer_data.append(last_item)\n",
    "        \n",
    "        second_layer_data.append((x, y))\n",
    "        last_target = target\n",
    "    \n",
    "    # Train last model\n",
    "    assert second_layer_data\n",
    "    container = RMITrainingData(second_layer_data)\n",
    "    leaf_model = train_model(model_type, container)\n",
    "    leaf_models.append(leaf_model)\n",
    "    \n",
    "    # Add remaining empty models\n",
    "    for _ in range(last_target + 1, first_model_idx + num_models):\n",
    "        empty_data = RMITrainingData([])\n",
    "        leaf_models.append(train_model(model_type, empty_data))\n",
    "    \n",
    "    assert len(leaf_models) == num_models\n",
    "    return leaf_models\n",
    "\n",
    "def train_two_layer(\n",
    "    data: RMITrainingData,\n",
    "    layer1_model: str,\n",
    "    layer2_model: str,\n",
    "    num_leaf_models: int\n",
    ") -> TrainedRMI:\n",
    "    \"\"\"From: rmi_lib/src/train/two_layer.rs\"\"\"\n",
    "    \n",
    "    num_rows = len(data)\n",
    "    \n",
    "    print(f\"Training top-level {layer1_model} model...\")\n",
    "    data.set_scale(num_leaf_models / num_rows)\n",
    "    top_model = train_model(layer1_model, data)\n",
    "    \n",
    "    print(f\"Training second-level {layer2_model} models (num models = {num_leaf_models})...\")\n",
    "    data.set_scale(1.0)\n",
    "    \n",
    "    # Find split point near middle\n",
    "    midpoint_model = num_leaf_models // 2\n",
    "    split_idx = data.lower_bound_by(\n",
    "        lambda x: -1 if top_model.predict_to_int(ModelInput(x[0])) < midpoint_model \n",
    "                  else (1 if top_model.predict_to_int(ModelInput(x[0])) > midpoint_model else 0)\n",
    "    )\n",
    "    \n",
    "    # Build leaf models\n",
    "    if split_idx >= len(data):\n",
    "        leaf_models = build_models_from(\n",
    "            data, top_model, layer2_model, 0, len(data), 0, num_leaf_models\n",
    "        )\n",
    "    else:\n",
    "        split_idx_target = min(\n",
    "            num_leaf_models - 1,\n",
    "            top_model.predict_to_int(ModelInput(data.get_key(split_idx)))\n",
    "        )\n",
    "        \n",
    "        first_half_models = split_idx_target\n",
    "        second_half_models = num_leaf_models - split_idx_target\n",
    "        \n",
    "        # Build first half\n",
    "        hf1 = build_models_from(\n",
    "            data, top_model, layer2_model, 0, split_idx, 0, first_half_models\n",
    "        )\n",
    "        \n",
    "        # Build second half\n",
    "        hf2 = build_models_from(\n",
    "            data, top_model, layer2_model, split_idx + 1, len(data),\n",
    "            split_idx_target, second_half_models\n",
    "        )\n",
    "        \n",
    "        leaf_models = hf1 + hf2\n",
    "    \n",
    "    print(\"Computing lower bound corrections...\")\n",
    "    lb_corrections = LowerBoundCorrection(\n",
    "        lambda x: top_model.predict_to_int(ModelInput(x)),\n",
    "        num_leaf_models,\n",
    "        data\n",
    "    )\n",
    "    \n",
    "    print(\"Fixing empty models...\")\n",
    "    # Replace empty models with constants\n",
    "    for idx in range(num_leaf_models - 1):\n",
    "        if lb_corrections.first_key(idx) is None:\n",
    "            upper_bound = lb_corrections.next_index(idx)\n",
    "            leaf_models[idx].set_to_constant_model(upper_bound)\n",
    "    \n",
    "    print(\"Computing last level errors...\")\n",
    "    # Compute errors for each leaf model\n",
    "    last_layer_max_l1s = [(0, 0)] * num_leaf_models\n",
    "    \n",
    "    for x, y in data.iter_model_input():\n",
    "        leaf_idx = top_model.predict_to_int(x)\n",
    "        target = min(num_leaf_models - 1, leaf_idx)\n",
    "        \n",
    "        pred = leaf_models[target].predict_to_int(x)\n",
    "        err = error_between(pred, y, len(data))\n",
    "        \n",
    "        cur_count, cur_max = last_layer_max_l1s[target]\n",
    "        last_layer_max_l1s[target] = (cur_count + 1, max(err, cur_max))\n",
    "    \n",
    "    # Adjust errors for lower bound correctness\n",
    "    for leaf_idx in range(num_leaf_models):\n",
    "        curr_err = last_layer_max_l1s[leaf_idx][1]\n",
    "        \n",
    "        # Upper error\n",
    "        idx_of_next, key_of_next = lb_corrections.next[leaf_idx]\n",
    "        key_minus_eps = key_of_next - (1 if isinstance(key_of_next, int) else 1e-10)\n",
    "        pred = leaf_models[leaf_idx].predict_to_int(ModelInput(key_minus_eps))\n",
    "        upper_error = error_between(pred, idx_of_next + 1, len(data))\n",
    "        \n",
    "        # Lower error\n",
    "        first_key_before = lb_corrections.prev_key(leaf_idx)\n",
    "        prev_idx = 0 if leaf_idx == 0 else leaf_idx - 1\n",
    "        first_idx = lb_corrections.next_index(prev_idx)\n",
    "        \n",
    "        key_plus_eps = first_key_before + (1 if isinstance(first_key_before, int) else 1e-10)\n",
    "        pred = leaf_models[leaf_idx].predict_to_int(ModelInput(key_plus_eps))\n",
    "        lower_error = error_between(pred, first_idx, len(data))\n",
    "        \n",
    "        new_err = max(curr_err, upper_error, lower_error) + lb_corrections.longest_run(leaf_idx)\n",
    "        \n",
    "        num_items = last_layer_max_l1s[leaf_idx][0]\n",
    "        last_layer_max_l1s[leaf_idx] = (num_items, new_err)\n",
    "    \n",
    "    print(\"Evaluating RMI...\")\n",
    "    # Compute statistics\n",
    "    model_max_error_idx, (_, model_max_error) = max(\n",
    "        enumerate(last_layer_max_l1s), key=lambda x: x[1][1]\n",
    "    )\n",
    "    \n",
    "    total_items = sum(n for n, _ in last_layer_max_l1s)\n",
    "    model_avg_error = sum(n * err for n, err in last_layer_max_l1s) / total_items\n",
    "    model_avg_l2_error = sum((n * err) ** 2 for n, err in last_layer_max_l1s) / total_items\n",
    "    model_avg_log2_error = sum(n * math.log2(2 * err + 2) for n, err in last_layer_max_l1s) / total_items\n",
    "    model_max_log2_error = math.log2(model_max_error) if model_max_error > 0 else 0.0\n",
    "    \n",
    "    final_errors = [err for _, err in last_layer_max_l1s]\n",
    "    \n",
    "    return TrainedRMI(\n",
    "        num_rmi_rows=len(data),\n",
    "        num_data_rows=len(data),\n",
    "        model_avg_error=model_avg_error,\n",
    "        model_avg_l2_error=model_avg_l2_error,\n",
    "        model_avg_log2_error=model_avg_log2_error,\n",
    "        model_max_error=model_max_error,\n",
    "        model_max_error_idx=model_max_error_idx,\n",
    "        model_max_log2_error=model_max_log2_error,\n",
    "        last_layer_max_l1s=final_errors,\n",
    "        rmi=[[top_model], leaf_models],\n",
    "        models=f\"{layer1_model},{layer2_model}\",\n",
    "        branching_factor=num_leaf_models,\n",
    "        build_time=0.0\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# From: rmi_lib/src/train/mod.rs\n",
    "# Main training function\n",
    "# =============================================================================\n",
    "\n",
    "def train(data: RMITrainingData, model_spec: str, branch_factor: int) -> TrainedRMI:\n",
    "    \"\"\"From: rmi_lib/src/train/mod.rs\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_list = model_spec.split(',')\n",
    "    if len(model_list) != 2:\n",
    "        raise ValueError(\"Only two-layer RMIs are currently supported\")\n",
    "    \n",
    "    layer1_model, layer2_model = model_list\n",
    "    \n",
    "    result = train_two_layer(data, layer1_model, layer2_model, branch_factor)\n",
    "    \n",
    "    build_time = time.time() - start_time\n",
    "    result.build_time = build_time\n",
    "    \n",
    "    print(f\"\\nRMI Training Complete!\")\n",
    "    print(f\"Build time: {build_time:.3f}s\")\n",
    "    print(f\"Average error: {result.model_avg_error:.2f}\")\n",
    "    print(f\"Max error: {result.model_max_error}\")\n",
    "    print(f\"Average log2 error: {result.model_avg_log2_error:.3f}\")\n",
    "    print(f\"Max log2 error: {result.model_max_log2_error:.3f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# =============================================================================\n",
    "# Example usage and testing\n",
    "# =============================================================================\n",
    "\n",
    "def load_binary_data(filename: str) -> RMITrainingData:\n",
    "    \"\"\"\n",
    "    Load binary data file in the format expected by RMI:\n",
    "    - First 8 bytes: number of items (uint64, little endian)\n",
    "    - Remaining bytes: data items (uint64 or uint32, little endian)\n",
    "    \"\"\"\n",
    "    import struct\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read number of items\n",
    "        num_items_bytes = f.read(8)\n",
    "        num_items = struct.unpack('<Q', num_items_bytes)[0]\n",
    "        \n",
    "        # Determine data type from filename\n",
    "        if 'uint32' in filename:\n",
    "            fmt = '<I'\n",
    "            item_size = 4\n",
    "        elif 'uint64' in filename:\n",
    "            fmt = '<Q'\n",
    "            item_size = 8\n",
    "        else:\n",
    "            raise ValueError(\"Filename must contain 'uint32' or 'uint64'\")\n",
    "        \n",
    "        # Read all data\n",
    "        data = []\n",
    "        for i in range(num_items):\n",
    "            item_bytes = f.read(item_size)\n",
    "            if len(item_bytes) < item_size:\n",
    "                break\n",
    "            value = struct.unpack(fmt, item_bytes)[0]\n",
    "            data.append((value, i))\n",
    "    \n",
    "    return RMITrainingData(data)\n",
    "\n",
    "def create_synthetic_data(n: int, distribution: str = 'uniform') -> RMITrainingData:\n",
    "    \"\"\"Create synthetic sorted data for testing\"\"\"\n",
    "    \n",
    "    if distribution == 'uniform':\n",
    "        keys = np.sort(np.random.randint(0, n * 10, size=n))\n",
    "    elif distribution == 'normal':\n",
    "        keys = np.sort(np.random.normal(n * 5, n, size=n).astype(int))\n",
    "        keys = np.maximum(keys, 0)  # Ensure non-negative\n",
    "    elif distribution == 'lognormal':\n",
    "        keys = np.sort(np.random.lognormal(10, 2, size=n).astype(int))\n",
    "    elif distribution == 'exponential':\n",
    "        keys = np.sort(np.random.exponential(n / 10, size=n).astype(int))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distribution: {distribution}\")\n",
    "    \n",
    "    # Create (key, position) pairs\n",
    "    data = [(int(k), i) for i, k in enumerate(keys)]\n",
    "    return RMITrainingData(data)\n",
    "\n",
    "def benchmark_rmi(rmi: TrainedRMI, test_keys: List[int], actual_data: List[Tuple[int, int]]):\n",
    "    \"\"\"\n",
    "    Benchmark RMI performance\n",
    "    From: general benchmarking approach\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Benchmarking RMI ===\")\n",
    "    \n",
    "    # Create sorted array for binary search comparison\n",
    "    sorted_keys = [k for k, _ in actual_data]\n",
    "    \n",
    "    # Test predictions\n",
    "    total_search_range = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for key in test_keys:\n",
    "        pred, err = rmi.predict(key)\n",
    "        \n",
    "        # Check if prediction is within error bound\n",
    "        lower = max(0, pred - err)\n",
    "        upper = min(len(actual_data) - 1, pred + err)\n",
    "        total_search_range += (upper - lower + 1)\n",
    "        \n",
    "        # Find actual position\n",
    "        actual_pos = bisect.bisect_left(sorted_keys, key)\n",
    "        \n",
    "        if lower <= actual_pos <= upper:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    avg_search_range = total_search_range / len(test_keys)\n",
    "    accuracy = 100 * correct_predictions / len(test_keys)\n",
    "    \n",
    "    print(f\"Tested {len(test_keys)} keys\")\n",
    "    print(f\"Average search range: {avg_search_range:.2f}\")\n",
    "    print(f\"Prediction accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Avg log2(search range): {math.log2(avg_search_range + 1):.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'avg_search_range': avg_search_range,\n",
    "        'accuracy': accuracy,\n",
    "        'total_tested': len(test_keys)\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Main demonstration\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Demonstrate RMI training and usage\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"RMI (Recursive Model Index) Implementation\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create synthetic dataset\n",
    "    print(\"\\n1. Creating synthetic dataset...\")\n",
    "    n = 1_000_000\n",
    "    data = create_synthetic_data(n, distribution='normal')\n",
    "    print(f\"Created {len(data)} data points\")\n",
    "    \n",
    "    # Train RMI with different configurations\n",
    "    configs = [\n",
    "        (\"linear,linear\", 1000),\n",
    "        (\"robust_linear,linear\", 1000),\n",
    "        (\"cubic,linear\", 1000),\n",
    "        (\"linear_spline,linear\", 1000),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model_spec, branch_factor in configs:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"2. Training RMI: {model_spec} with branching factor {branch_factor}\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        rmi = train(data.soft_copy(), model_spec, branch_factor)\n",
    "        \n",
    "        # Test on random keys\n",
    "        print(\"\\n3. Testing predictions...\")\n",
    "        test_keys = [data.get_key(i) for i in np.random.randint(0, len(data), size=1000)]\n",
    "        \n",
    "        benchmark_results = benchmark_rmi(rmi, test_keys, data.data)\n",
    "        \n",
    "        results.append({\n",
    "            'config': model_spec,\n",
    "            'branching_factor': branch_factor,\n",
    "            'build_time': rmi.build_time,\n",
    "            'avg_error': rmi.model_avg_error,\n",
    "            'max_error': rmi.model_max_error,\n",
    "            'avg_log2_error': rmi.model_avg_log2_error,\n",
    "            'avg_search_range': benchmark_results['avg_search_range']\n",
    "        })\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RESULTS COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Config':<25} {'Branch':<8} {'Build(s)':<10} {'AvgErr':<10} {'MaxErr':<10} {'AvgLog2':<10} {'SearchRng':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['config']:<25} {r['branching_factor']:<8} {r['build_time']:<10.3f} \"\n",
    "              f\"{r['avg_error']:<10.2f} {r['max_error']:<10} {r['avg_log2_error']:<10.3f} \"\n",
    "              f\"{r['avg_search_range']:<10.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Done!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef73e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Dataset Downloader\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Available SOSD Datasets ===\n",
      "\n",
      "books_200M_uint64\n",
      "  Status:  Downloaded\n",
      "  Description: Amazon book popularity (sorted by popularity rank)\n",
      "  Records: 200,000,000\n",
      "  Size: 1.60 GB\n",
      "\n",
      "fb_200M_uint64\n",
      "  Status:  Downloaded\n",
      "  Description: Facebook user IDs\n",
      "  Records: 200,000,000\n",
      "  Size: 1.60 GB\n",
      "\n",
      "osm_cellids_200M_uint64\n",
      "  Status:  Not downloaded\n",
      "  Description: OpenStreetMap cell IDs\n",
      "  Records: 200,000,000\n",
      "  Size: 1.60 GB\n",
      "\n",
      "wiki_ts_200M_uint64\n",
      "  Status:  Not downloaded\n",
      "  Description: Wikipedia edit timestamps\n",
      "  Records: 200,000,000\n",
      "  Size: 1.60 GB\n",
      "\n",
      "\n",
      "Example 2: Load and Inspect Dataset\n",
      "--------------------------------------------------\n",
      "# To load a dataset:\n",
      "Loading dataset: fb_200M_uint64\n",
      "Total records in file: 1,000,000\n",
      "Reading data...\n",
      "Loaded 1,000,000 records\n",
      "Sorting data...\n",
      " Dataset ready: 1,000,000 records\n",
      "First 10 keys: [200000000, 979672113, 1000931008, 1013842213, 1023286855, 1027557666, 1029155329, 1039927090, 1044144712, 1045191996]\n",
      "\n",
      "\n",
      "Example 3: Run Comprehensive Benchmark\n",
      "--------------------------------------------------\n",
      "Uncomment the line below to run:\n",
      "======================================================================\n",
      "RMI Comprehensive Benchmark Suite\n",
      "======================================================================\n",
      "\n",
      "1. Creating synthetic datasets...\n",
      "\n",
      "======================================================================\n",
      "Benchmarking: uniform_1M.dat\n",
      "======================================================================\n",
      "Loading dataset: uniform_1M.dat\n",
      "Total records in file: 100,000\n",
      "Reading data...\n",
      "Loaded 100,000 records\n",
      "Sorting data...\n",
      " Dataset ready: 100,000 records\n",
      "\n",
      "--- Testing: linear,linear (branch=1000) ---\n",
      "=== Benchmark Result ===\n",
      "Dataset: uniform_1M (100,000 records)\n",
      "Model: linear,linear (branch=1000)\n",
      "\n",
      "Build Time: 0.329s\n",
      "Average Error: 0.00\n",
      "Max Error: 0\n",
      "Avg Log2 Error: 1.000\n",
      "\n",
      "Average Search Range: 1.00\n",
      "Prediction Accuracy: 100.0%\n",
      "Log2(Search Range): 1.000\n",
      "\n",
      "\n",
      "--- Testing: linear,linear (branch=5000) ---\n",
      "=== Benchmark Result ===\n",
      "Dataset: uniform_1M (100,000 records)\n",
      "Model: linear,linear (branch=5000)\n",
      "\n",
      "Build Time: 0.558s\n",
      "Average Error: 0.00\n",
      "Max Error: 0\n",
      "Avg Log2 Error: 1.000\n",
      "\n",
      "Average Search Range: 1.00\n",
      "Prediction Accuracy: 100.0%\n",
      "Log2(Search Range): 1.000\n",
      "\n",
      "\n",
      "--- Testing: linear,linear (branch=10000) ---\n",
      "=== Benchmark Result ===\n",
      "Dataset: uniform_1M (100,000 records)\n",
      "Model: linear,linear (branch=10000)\n",
      "\n",
      "Build Time: 0.973s\n",
      "Average Error: 0.00\n",
      "Max Error: 0\n",
      "Avg Log2 Error: 1.000\n",
      "\n",
      "Average Search Range: 1.00\n",
      "Prediction Accuracy: 100.0%\n",
      "Log2(Search Range): 1.000\n",
      "\n",
      "\n",
      "Found real dataset: books_200M_uint64\n",
      "\n",
      "======================================================================\n",
      "Benchmarking: books_200M_uint64\n",
      "======================================================================\n",
      "Loading dataset: books_200M_uint64\n",
      "Total records in file: 1,000,000\n",
      "Reading data...\n",
      "Loaded 1,000,000 records\n",
      "Sorting data...\n",
      " Dataset ready: 1,000,000 records\n",
      "\n",
      "--- Testing: linear,linear (branch=1000) ---\n",
      "=== Benchmark Result ===\n",
      "Dataset: books_200M_uint64 (1,000,000 records)\n",
      "Model: linear,linear (branch=1000)\n",
      "\n",
      "Build Time: 2.909s\n",
      "Average Error: 20.19\n",
      "Max Error: 38\n",
      "Avg Log2 Error: 5.367\n",
      "\n",
      "Average Search Range: 41.43\n",
      "Prediction Accuracy: 100.0%\n",
      "Log2(Search Range): 5.407\n",
      "\n",
      "\n",
      "--- Testing: linear,linear (branch=5000) ---\n",
      "=== Benchmark Result ===\n",
      "Dataset: books_200M_uint64 (1,000,000 records)\n",
      "Model: linear,linear (branch=5000)\n",
      "\n",
      "Build Time: 3.344s\n",
      "Average Error: 8.66\n",
      "Max Error: 21\n",
      "Avg Log2 Error: 4.233\n",
      "\n",
      "Average Search Range: 18.22\n",
      "Prediction Accuracy: 100.0%\n",
      "Log2(Search Range): 4.265\n",
      "\n",
      "\n",
      "--- Testing: linear,linear (branch=10000) ---\n",
      "=== Benchmark Result ===\n",
      "Dataset: books_200M_uint64 (1,000,000 records)\n",
      "Model: linear,linear (branch=10000)\n",
      "\n",
      "Build Time: 3.322s\n",
      "Average Error: 5.99\n",
      "Max Error: 17\n",
      "Avg Log2 Error: 3.768\n",
      "\n",
      "Average Search Range: 13.15\n",
      "Prediction Accuracy: 100.0%\n",
      "Log2(Search Range): 3.823\n",
      "\n",
      "\n",
      "Found real dataset: fb_200M_uint64\n",
      "\n",
      "======================================================================\n",
      "Benchmarking: fb_200M_uint64\n",
      "======================================================================\n",
      "Loading dataset: fb_200M_uint64\n",
      "Total records in file: 1,000,000\n",
      "Reading data...\n",
      "Loaded 1,000,000 records\n",
      "Sorting data...\n",
      " Dataset ready: 1,000,000 records\n",
      "\n",
      "--- Testing: linear,linear (branch=1000) ---\n",
      "=== Benchmark Result ===\n",
      "Dataset: fb_200M_uint64 (1,000,000 records)\n",
      "Model: linear,linear (branch=1000)\n",
      "\n",
      "Build Time: 2.815s\n",
      "Average Error: 486.49\n",
      "Max Error: 3,267\n",
      "Avg Log2 Error: 7.105\n",
      "\n",
      "Average Search Range: 2855.31\n",
      "Prediction Accuracy: 100.0%\n",
      "Log2(Search Range): 11.480\n",
      "\n",
      "\n",
      "--- Testing: linear,linear (branch=5000) ---\n",
      "=== Benchmark Result ===\n",
      "Dataset: fb_200M_uint64 (1,000,000 records)\n",
      "Model: linear,linear (branch=5000)\n",
      "\n",
      "Build Time: 2.765s\n",
      "Average Error: 67.26\n",
      "Max Error: 2,413\n",
      "Avg Log2 Error: 4.127\n",
      "\n",
      "Average Search Range: 1074.35\n",
      "Prediction Accuracy: 100.0%\n",
      "Log2(Search Range): 10.071\n",
      "\n",
      "\n",
      "--- Testing: linear,linear (branch=10000) ---\n",
      "=== Benchmark Result ===\n",
      "Dataset: fb_200M_uint64 (1,000,000 records)\n",
      "Model: linear,linear (branch=10000)\n",
      "\n",
      "Build Time: 3.002s\n",
      "Average Error: 24.73\n",
      "Max Error: 1,693\n",
      "Avg Log2 Error: 3.452\n",
      "\n",
      "Average Search Range: 520.86\n",
      "Prediction Accuracy: 100.0%\n",
      "Log2(Search Range): 9.028\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "BENCHMARK COMPARISON\n",
      "========================================================================================================================\n",
      "Dataset                   Model                Branch   Build(s)   AvgErr       SearchRng    Accuracy  \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "uniform_1M                linear,linear        1000     0.329      0.00         1.00         100.0     %\n",
      "uniform_1M                linear,linear        5000     0.558      0.00         1.00         100.0     %\n",
      "uniform_1M                linear,linear        10000    0.973      0.00         1.00         100.0     %\n",
      "books_200M_uint64         linear,linear        1000     2.909      20.19        41.43        100.0     %\n",
      "books_200M_uint64         linear,linear        5000     3.344      8.66         18.22        100.0     %\n",
      "books_200M_uint64         linear,linear        10000    3.322      5.99         13.15        100.0     %\n",
      "fb_200M_uint64            linear,linear        1000     2.815      486.49       2855.31      100.0     %\n",
      "fb_200M_uint64            linear,linear        5000     2.765      67.26        1074.35      100.0     %\n",
      "fb_200M_uint64            linear,linear        10000    3.002      24.73        520.86       100.0     %\n",
      "\n",
      " Results saved to benchmark_results\\benchmark_results.json\n",
      "\n",
      "======================================================================\n",
      "Benchmark Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RMI Implementation with Real Dataset Support\n",
    "Optimized for SOSD Benchmark and Real-World Data\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Union\n",
    "import time\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET DOWNLOADING AND LOADING\n",
    "# =============================================================================\n",
    "\n",
    "class SOSDDatasetDownloader:\n",
    "    \"\"\"\n",
    "    Download and manage SOSD benchmark datasets\n",
    "    Source: https://github.com/learnedsystems/SOSD\n",
    "    \"\"\"\n",
    "    \n",
    "    DATASETS = {\n",
    "        'books_200M_uint64': {\n",
    "            'url': 'https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/JGVF9A/MZZUP2',\n",
    "            'size': 1600000000,  # bytes\n",
    "            'records': 200000000,\n",
    "            'description': 'Amazon book popularity (sorted by popularity rank)'\n",
    "        },\n",
    "        'fb_200M_uint64': {\n",
    "            'url': 'https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/JGVF9A/SVN8PI',\n",
    "            'size': 1600000000,\n",
    "            'records': 200000000,\n",
    "            'description': 'Facebook user IDs'\n",
    "        },\n",
    "        'osm_cellids_200M_uint64': {\n",
    "            'url': 'https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/JGVF9A/LMTZJA',\n",
    "            'size': 1600000000,\n",
    "            'records': 200000000,\n",
    "            'description': 'OpenStreetMap cell IDs'\n",
    "        },\n",
    "        'wiki_ts_200M_uint64': {\n",
    "            'url': 'https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/JGVF9A/HJPVBB',\n",
    "            'size': 1600000000,\n",
    "            'records': 200000000,\n",
    "            'description': 'Wikipedia edit timestamps'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, data_dir: str = './sosd_data'):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def download_dataset(self, dataset_name: str, force: bool = False):\n",
    "        \"\"\"Download a specific dataset\"\"\"\n",
    "        if dataset_name not in self.DATASETS:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset_name}. Available: {list(self.DATASETS.keys())}\")\n",
    "        \n",
    "        dataset_info = self.DATASETS[dataset_name]\n",
    "        filepath = self.data_dir / dataset_name\n",
    "        \n",
    "        if filepath.exists() and not force:\n",
    "            print(f\"Dataset {dataset_name} already exists. Use force=True to re-download.\")\n",
    "            return filepath\n",
    "        \n",
    "        print(f\"Downloading {dataset_name}...\")\n",
    "        print(f\"Description: {dataset_info['description']}\")\n",
    "        print(f\"Size: {dataset_info['size'] / 1e9:.2f} GB\")\n",
    "        print(f\"Records: {dataset_info['records']:,}\")\n",
    "        \n",
    "        # Download with progress bar\n",
    "        response = requests.get(dataset_info['url'], stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        progress = (downloaded / total_size) * 100\n",
    "                        print(f\"\\rProgress: {progress:.1f}%\", end='', flush=True)\n",
    "        \n",
    "        print(f\"\\n Downloaded {dataset_name} to {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def list_available(self):\n",
    "        \"\"\"List all available datasets\"\"\"\n",
    "        print(\"\\n=== Available SOSD Datasets ===\")\n",
    "        for name, info in self.DATASETS.items():\n",
    "            exists = (self.data_dir / name).exists()\n",
    "            status = \" Downloaded\" if exists else \" Not downloaded\"\n",
    "            print(f\"\\n{name}\")\n",
    "            print(f\"  Status: {status}\")\n",
    "            print(f\"  Description: {info['description']}\")\n",
    "            print(f\"  Records: {info['records']:,}\")\n",
    "            print(f\"  Size: {info['size'] / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def load_binary_dataset(filepath: str, max_records: Optional[int] = None) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Load binary dataset in SOSD format:\n",
    "    - First 8 bytes: number of items (uint64, little endian)\n",
    "    - Remaining bytes: data items (uint64 or uint32, little endian)\n",
    "    \n",
    "    Returns: List of (key, position) tuples\n",
    "    \"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found: {filepath}\")\n",
    "    \n",
    "    # Determine data type from filename\n",
    "    if 'uint32' in filepath.name:\n",
    "        dtype = np.uint32\n",
    "        item_size = 4\n",
    "    elif 'uint64' in filepath.name or 'f64' in filepath.name:\n",
    "        dtype = np.uint64\n",
    "        item_size = 8\n",
    "    else:\n",
    "        # Default to uint64\n",
    "        dtype = np.uint64\n",
    "        item_size = 8\n",
    "    \n",
    "    print(f\"Loading dataset: {filepath.name}\")\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        # Read number of items\n",
    "        num_items_bytes = f.read(8)\n",
    "        num_items = struct.unpack('<Q', num_items_bytes)[0]\n",
    "        \n",
    "        if max_records:\n",
    "            num_items = min(num_items, max_records)\n",
    "        \n",
    "        print(f\"Total records in file: {num_items:,}\")\n",
    "        \n",
    "        # Read data items\n",
    "        print(\"Reading data...\")\n",
    "        data = np.fromfile(f, dtype=dtype, count=num_items)\n",
    "    \n",
    "    print(f\"Loaded {len(data):,} records\")\n",
    "    \n",
    "    # Sort and create (key, position) pairs\n",
    "    print(\"Sorting data...\")\n",
    "    sorted_indices = np.argsort(data)\n",
    "    sorted_data = data[sorted_indices]\n",
    "    \n",
    "    # Create position mapping\n",
    "    result = [(int(key), int(pos)) for pos, key in enumerate(sorted_data)]\n",
    "    \n",
    "    print(f\" Dataset ready: {len(result):,} records\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_csv_dataset(filepath: str, key_column: int = 0, max_records: Optional[int] = None) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Load CSV dataset and convert to RMI format\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file\n",
    "        key_column: Which column to use as key (0-indexed)\n",
    "        max_records: Maximum number of records to load\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(f\"Loading CSV: {filepath}\")\n",
    "    df = pd.read_csv(filepath, nrows=max_records)\n",
    "    \n",
    "    # Get key column\n",
    "    keys = df.iloc[:, key_column].values\n",
    "    \n",
    "    # Sort and create positions\n",
    "    sorted_indices = np.argsort(keys)\n",
    "    sorted_keys = keys[sorted_indices]\n",
    "    \n",
    "    result = [(int(key), int(pos)) for pos, key in enumerate(sorted_keys)]\n",
    "    print(f\" Loaded {len(result):,} records from CSV\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZED RMI IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class RMITrainingData:\n",
    "    \"\"\"Optimized training data with NumPy backend\"\"\"\n",
    "    def __init__(self, data: List[Tuple[Union[int, float], int]]):\n",
    "        # Convert to NumPy arrays for performance\n",
    "        self.keys = np.array([k for k, _ in data])\n",
    "        self.positions = np.array([p for _, p in data])\n",
    "        self.scale = 1.0\n",
    "        \n",
    "        # Sort by keys\n",
    "        sort_idx = np.argsort(self.keys)\n",
    "        self.keys = self.keys[sort_idx]\n",
    "        self.positions = self.positions[sort_idx]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def set_scale(self, scale: float):\n",
    "        self.scale = scale\n",
    "    \n",
    "    def get(self, idx: int) -> Tuple[Union[int, float], int]:\n",
    "        key = self.keys[idx]\n",
    "        pos = self.positions[idx]\n",
    "        if abs(self.scale - 1.0) > 1e-10:\n",
    "            pos = int(pos * self.scale)\n",
    "        return (key, pos)\n",
    "    \n",
    "    def get_key(self, idx: int):\n",
    "        return self.keys[idx]\n",
    "    \n",
    "    def iter(self):\n",
    "        \"\"\"Efficient iterator using NumPy\"\"\"\n",
    "        if abs(self.scale - 1.0) > 1e-10:\n",
    "            scaled_pos = (self.positions * self.scale).astype(int)\n",
    "            for k, p in zip(self.keys, scaled_pos):\n",
    "                yield (k, p)\n",
    "        else:\n",
    "            for k, p in zip(self.keys, self.positions):\n",
    "                yield (k, p)\n",
    "    \n",
    "    def soft_copy(self):\n",
    "        \"\"\"Create shallow copy\"\"\"\n",
    "        new_data = RMITrainingData.__new__(RMITrainingData)\n",
    "        new_data.keys = self.keys\n",
    "        new_data.positions = self.positions\n",
    "        new_data.scale = self.scale\n",
    "        return new_data\n",
    "\n",
    "\n",
    "class LinearModel:\n",
    "    \"\"\"Optimized linear regression using NumPy\"\"\"\n",
    "    def __init__(self, data: RMITrainingData):\n",
    "        if len(data) == 0:\n",
    "            self.intercept, self.slope = 0.0, 0.0\n",
    "            return\n",
    "        \n",
    "        if len(data) == 1:\n",
    "            _, y = data.get(0)\n",
    "            self.intercept, self.slope = float(y), 0.0\n",
    "            return\n",
    "        \n",
    "        # Vectorized linear regression\n",
    "        X = data.keys.astype(np.float64)\n",
    "        y = np.array([p for _, p in data.iter()], dtype=np.float64)\n",
    "        \n",
    "        if len(np.unique(X)) == 1:\n",
    "            self.intercept, self.slope = np.mean(y), 0.0\n",
    "            return\n",
    "        \n",
    "        # Compute slope and intercept\n",
    "        X_mean = np.mean(X)\n",
    "        y_mean = np.mean(y)\n",
    "        \n",
    "        numerator = np.sum((X - X_mean) * (y - y_mean))\n",
    "        denominator = np.sum((X - X_mean) ** 2)\n",
    "        \n",
    "        if denominator == 0:\n",
    "            self.slope = 0.0\n",
    "        else:\n",
    "            self.slope = numerator / denominator\n",
    "        \n",
    "        self.intercept = y_mean - self.slope * X_mean\n",
    "    \n",
    "    def predict(self, key: Union[int, float]) -> int:\n",
    "        return max(0, int(self.slope * float(key) + self.intercept))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE BENCHMARKING FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    dataset_name: str\n",
    "    dataset_size: int\n",
    "    model_config: str\n",
    "    branch_factor: int\n",
    "    \n",
    "    # Build metrics\n",
    "    build_time: float\n",
    "    \n",
    "    # Error metrics\n",
    "    avg_error: float\n",
    "    max_error: int\n",
    "    avg_log2_error: float\n",
    "    max_log2_error: float\n",
    "    \n",
    "    # Query performance\n",
    "    avg_search_range: float\n",
    "    prediction_accuracy: float\n",
    "    \n",
    "    # Memory\n",
    "    model_size_bytes: int\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dict with JSON-serializable types\"\"\"\n",
    "        result = asdict(self)\n",
    "        # Convert numpy types to native Python types\n",
    "        for key, value in result.items():\n",
    "            if hasattr(value, 'item'):  # NumPy scalar\n",
    "                result[key] = value.item()\n",
    "            elif isinstance(value, (np.integer, np.floating)):\n",
    "                result[key] = value.item()\n",
    "        return result\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        return f\"\"\"=== Benchmark Result ===\n",
    "Dataset: {self.dataset_name} ({self.dataset_size:,} records)\n",
    "Model: {self.model_config} (branch={self.branch_factor})\n",
    "\n",
    "Build Time: {self.build_time:.3f}s\n",
    "Average Error: {self.avg_error:.2f}\n",
    "Max Error: {self.max_error:,}\n",
    "Avg Log2 Error: {self.avg_log2_error:.3f}\n",
    "\n",
    "Average Search Range: {self.avg_search_range:.2f}\n",
    "Prediction Accuracy: {self.prediction_accuracy:.1f}%\n",
    "Log2(Search Range): {np.log2(self.avg_search_range + 1):.3f}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RMIBenchmark:\n",
    "    \"\"\"Comprehensive benchmarking framework\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = './benchmark_results'):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.results = []\n",
    "    \n",
    "    def benchmark_dataset(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        model_configs: List[Tuple[str, int]],\n",
    "        max_records: Optional[int] = None,\n",
    "        num_test_queries: int = 1000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Benchmark multiple RMI configurations on a dataset\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to dataset file\n",
    "            model_configs: List of (model_spec, branch_factor) tuples\n",
    "            max_records: Limit number of records (for testing)\n",
    "            num_test_queries: Number of queries for testing\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Benchmarking: {Path(dataset_path).name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        if dataset_path.endswith('.csv'):\n",
    "            data_list = load_csv_dataset(dataset_path, max_records=max_records)\n",
    "        else:\n",
    "            data_list = load_binary_dataset(dataset_path, max_records=max_records)\n",
    "        \n",
    "        data = RMITrainingData(data_list)\n",
    "        dataset_name = Path(dataset_path).stem\n",
    "        \n",
    "        # Generate test queries\n",
    "        test_indices = np.random.randint(0, len(data), size=num_test_queries)\n",
    "        test_keys = [data.get_key(i) for i in test_indices]\n",
    "        \n",
    "        # Benchmark each configuration\n",
    "        for model_spec, branch_factor in model_configs:\n",
    "            print(f\"\\n--- Testing: {model_spec} (branch={branch_factor}) ---\")\n",
    "            \n",
    "            try:\n",
    "                # Train RMI\n",
    "                start_time = time.time()\n",
    "                rmi = self._train_simple_rmi(data.soft_copy(), model_spec, branch_factor)\n",
    "                build_time = time.time() - start_time\n",
    "                \n",
    "                # Test queries\n",
    "                total_search_range = 0\n",
    "                correct = 0\n",
    "                \n",
    "                for key in test_keys:\n",
    "                    pred, err = rmi['predict'](key)\n",
    "                    \n",
    "                    lower = max(0, pred - err)\n",
    "                    upper = min(len(data) - 1, pred + err)\n",
    "                    search_range = upper - lower + 1\n",
    "                    total_search_range += search_range\n",
    "                    \n",
    "                    # Check correctness using binary search\n",
    "                    actual_pos = np.searchsorted(data.keys, key)\n",
    "                    if lower <= actual_pos <= upper:\n",
    "                        correct += 1\n",
    "                \n",
    "                avg_search_range = total_search_range / len(test_keys)\n",
    "                accuracy = 100 * correct / len(test_keys)\n",
    "                \n",
    "                # Create result\n",
    "                result = BenchmarkResult(\n",
    "                    dataset_name=dataset_name,\n",
    "                    dataset_size=len(data),\n",
    "                    model_config=model_spec,\n",
    "                    branch_factor=branch_factor,\n",
    "                    build_time=build_time,\n",
    "                    avg_error=rmi['avg_error'],\n",
    "                    max_error=rmi['max_error'],\n",
    "                    avg_log2_error=rmi['avg_log2_error'],\n",
    "                    max_log2_error=rmi['max_log2_error'],\n",
    "                    avg_search_range=avg_search_range,\n",
    "                    prediction_accuracy=accuracy,\n",
    "                    model_size_bytes=0  # TODO: calculate actual size\n",
    "                )\n",
    "                \n",
    "                print(result.summary())\n",
    "                self.results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Failed: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def _train_simple_rmi(self, data: RMITrainingData, model_spec: str, branch_factor: int):\n",
    "        \"\"\"Simplified 2-layer RMI training\"\"\"\n",
    "        layer1, layer2 = model_spec.split(',')\n",
    "        \n",
    "        # Train top model\n",
    "        data.set_scale(branch_factor / len(data))\n",
    "        top_model = LinearModel(data)\n",
    "        \n",
    "        # Train leaf models\n",
    "        data.set_scale(1.0)\n",
    "        leaf_models = []\n",
    "        leaf_errors = []\n",
    "        \n",
    "        # Partition data\n",
    "        partitions = [[] for _ in range(branch_factor)]\n",
    "        for key, pos in data.iter():\n",
    "            pred = top_model.predict(key)\n",
    "            target = min(branch_factor - 1, pred)\n",
    "            partitions[target].append((key, pos))\n",
    "        \n",
    "        # Train each leaf\n",
    "        for partition in partitions:\n",
    "            if len(partition) == 0:\n",
    "                leaf_models.append(LinearModel(RMITrainingData([(0, 0)])))\n",
    "                leaf_errors.append(0)\n",
    "            else:\n",
    "                leaf_data = RMITrainingData(partition)\n",
    "                leaf_model = LinearModel(leaf_data)\n",
    "                leaf_models.append(leaf_model)\n",
    "                \n",
    "                # Calculate error\n",
    "                max_err = 0\n",
    "                for key, pos in partition:\n",
    "                    pred = leaf_model.predict(key)\n",
    "                    err = abs(pred - pos)\n",
    "                    max_err = max(max_err, err)\n",
    "                leaf_errors.append(max_err)\n",
    "        \n",
    "        # Create prediction function\n",
    "        def predict(key):\n",
    "            pred = top_model.predict(key)\n",
    "            target = min(branch_factor - 1, pred)\n",
    "            leaf_pred = leaf_models[target].predict(key)\n",
    "            return (leaf_pred, leaf_errors[target])\n",
    "        \n",
    "        return {\n",
    "            'predict': predict,\n",
    "            'avg_error': np.mean(leaf_errors),\n",
    "            'max_error': max(leaf_errors),\n",
    "            'avg_log2_error': np.mean([np.log2(2*e + 2) for e in leaf_errors]),\n",
    "            'max_log2_error': np.log2(2*max(leaf_errors) + 2) if max(leaf_errors) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def save_results(self, filename: str = 'benchmark_results.json'):\n",
    "        \"\"\"Save all results to JSON\"\"\"\n",
    "        filepath = self.output_dir / filename\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump([r.to_dict() for r in self.results], f, indent=2)\n",
    "        print(f\"\\n Results saved to {filepath}\")\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print comparison table\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to compare\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"BENCHMARK COMPARISON\")\n",
    "        print(\"=\"*120)\n",
    "        print(f\"{'Dataset':<25} {'Model':<20} {'Branch':<8} {'Build(s)':<10} {'AvgErr':<12} \"\n",
    "              f\"{'SearchRng':<12} {'Accuracy':<10}\")\n",
    "        print(\"-\"*120)\n",
    "        \n",
    "        for r in self.results:\n",
    "            print(f\"{r.dataset_name:<25} {r.model_config:<20} {r.branch_factor:<8} \"\n",
    "                  f\"{r.build_time:<10.3f} {r.avg_error:<12.2f} {r.avg_search_range:<12.2f} \"\n",
    "                  f\"{r.prediction_accuracy:<10.1f}%\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "def run_comprehensive_benchmark():\n",
    "    \"\"\"Run comprehensive benchmark on multiple datasets\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"RMI Comprehensive Benchmark Suite\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize benchmark\n",
    "    benchmark = RMIBenchmark()\n",
    "    \n",
    "    # Model configurations to test\n",
    "    configs = [\n",
    "        (\"linear,linear\", 1000),\n",
    "        (\"linear,linear\", 5000),\n",
    "        (\"linear,linear\", 10000),\n",
    "    ]\n",
    "    \n",
    "    # Test on synthetic data first (fast)\n",
    "    print(\"\\n1. Creating synthetic datasets...\")\n",
    "    \n",
    "    # Uniform distribution\n",
    "    uniform_data = [(i, i) for i in range(1000000)]\n",
    "    uniform_path = Path('./test_data/uniform_1M.dat')\n",
    "    uniform_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save as binary\n",
    "    with open(uniform_path, 'wb') as f:\n",
    "        f.write(struct.pack('<Q', len(uniform_data)))\n",
    "        for key, _ in uniform_data:\n",
    "            f.write(struct.pack('<Q', key))\n",
    "    \n",
    "    benchmark.benchmark_dataset(\n",
    "        str(uniform_path),\n",
    "        configs,\n",
    "        max_records=100000,  # Use subset for speed\n",
    "        num_test_queries=1000\n",
    "    )\n",
    "    \n",
    "    # Test on real SOSD data if available\n",
    "    sosd_dir = Path('./sosd_data')\n",
    "    if sosd_dir.exists():\n",
    "        for dataset_file in sosd_dir.glob('*_uint64'):\n",
    "            print(f\"\\nFound real dataset: {dataset_file.name}\")\n",
    "            benchmark.benchmark_dataset(\n",
    "                str(dataset_file),\n",
    "                configs,\n",
    "                max_records=1000000,  # Use 1M records for faster testing\n",
    "                num_test_queries=1000\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\nNo SOSD datasets found. Run downloader to get real datasets.\")\n",
    "    \n",
    "    # Print results\n",
    "    benchmark.print_comparison()\n",
    "    benchmark.save_results()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Benchmark Complete!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Download SOSD datasets\n",
    "    print(\"Example 1: Dataset Downloader\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    downloader = SOSDDatasetDownloader()\n",
    "    downloader.list_available()\n",
    "    \n",
    "    # downloader.download_dataset('fb_200M_uint64')\n",
    "    \n",
    "    # Example 2: Load and inspect a dataset\n",
    "    print(\"\\n\\nExample 2: Load and Inspect Dataset\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"# To load a dataset:\")\n",
    "    data = load_binary_dataset('./sosd_data/fb_200M_uint64', max_records=1000000)\n",
    "    print(f'First 10 keys: {[k for k, _ in data[:10]]}')\n",
    "    \n",
    "    # Example 3: Run benchmark\n",
    "    print(\"\\n\\nExample 3: Run Comprehensive Benchmark\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Uncomment the line below to run:\")\n",
    "    run_comprehensive_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf585648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
